#!/bin/bash
#
# ssh1 - script to connect to an existing Scylla cluster + loaders previous
# created with SCT using the script nyh/run2a3, and run a benchmark.
#
# What exactly ssh1 runs can be chosen with environment variables:
#
# * (unconditional right now) Run write throughput benchmark
# * isolation:  set to "forbid_rmw" for writes without LWT (the default),
#               or to "always_use_lwt" (with LWT).
# * aws:        if empty (default), benchmark the Scylla servers.
#               If "aws is set, we test the real DynamoDB in that Amazon
#               region (e.g., "aws=us-east-1").
# * wcu:        The WCU (write capacity units) to create the table in
#               provisioned billing mode. without it, use on-demand billing
#               mode. Ignored in !aws mode.
#               wcu is also used to set the maximum rate sent by each loader
#               to wcu / number_of_loaders.
# * fieldcount, fieldlength: Control the size of each item, as fieldcount
#               attributes of size fieldlength each. The defaults are 10,256.
# * time:       how much time (in seconds) to run the test. Defaults to 300.
# * usertable:  how to call the table used for the YCSB test. This defaults
#               to usertable_$(logname), but if you want to run multiple
#               tests in parallel, you may need to change this.

# take various ec2-* scripts from the current directory
PATH=.:$PATH

set -ex

# Write Isolation Mode to use in the test - "forbid_rmw" (no LWT) or
# "always_use_lwt":
isolation=${isolation-forbid_rmw}

# Table name to be used in the test. Because when running on AWS DynamoDB
# table names are shared by all users, let's use the user name as part of
# the table name, to allow more than one user to run a benchmark in
# parallel. Also allow to override the choice of name, in case the same
# user wants to run multiple benchmarks in parallel they need to choose
# a different table name for each test. We could use unique table names
# for each test automatically, but that will increase the risk of leaving
# expensive tables behind.
usertable=${usertable-usertable_`logname`}

# If "aws" environment variable is set, we test the real DynamoDB in that
# Amazon region (e.g., "aws=us-east-1").
# When "aws" is set it is recommended to also set "wcu" to have the test
# provision the table and not just used on-demand (pay-per-request).
#
# If "aws" is empty, Scylla (on nodes returned by ec2-ips) is contacted.

ec2-refresh

# Set up loader machines
loaders=(`ec2-ips loader`)

case $aws in
"") # Scylla
    # Modified YCSB using ~/.aws/credentials and not a separate file
    cat <<END >/tmp/alternator-credentials
    aws_access_key_id = alternator
    aws_secret_access_key = alternator_secret_access_key
END
    ;;
*) # AWS
    ./ec2-temporary-key >/tmp/alternator-credentials
    ;;
esac

for loader in ${loaders[@]}
do
    (
        ec2-ssh $loader mkdir -p .aws
        ec2-scp /tmp/alternator-credentials $loader:.aws/credentials
    )&
done
wait

#SCT's YCSB:
#YCSB="cd ycsb-0.15.0; bin/ycsb -jvm-args='-Dorg.slf4j.simpleLogger.defaultLogLevel=OFF'"

# Copy my version of YCSB:
# To build ~/YCSB/OUT, do:
#   cd $HOME
#   git clone git@github.com:brianfrankcooper/YCSB.git
#   cd YCSB
#   mvn -pl site.ycsb:dynamodb-binding -am clean package -DskipTests
#   mkdir OUT
#   tar zxvf dynamodb/target/ycsb-dynamodb-binding* -C OUT --strip-components 1
MY_YCSB_DIR=${MY_YCSB_DIR-/home/nyh/YCSB/OUT}
YCSB="cd ycsb; bin/ycsb"
for loader in ${loaders[@]}
do
    (
        ec2-rsync -aP $MY_YCSB_DIR/ $loader:ycsb/
    )&
done
wait

case $aws in
"") # Scylla
    # Pick one of the Scylla nodes as the one to send administrative
    # requests to:
    scylla_api=http://`ec2-ips scylla-db | head -1`:8080
    AWS_CMD=aws
    ;;
*)  # AWS
    scylla_api=http://dynamodb.$aws.amazonaws.com
    AWS_CMD="aws --region $aws"
    ;;
esac

# Create the test table "usertable" (deleting it first if it exists)
# FIXME: need to delete the table without snapshots, which just wastes
# time on the server! If we can't configure Scylla properly, at least
# ssh to the server and delete the snapshots :-(
$AWS_CMD dynamodb delete-table \
    --endpoint-url $scylla_api \
    --table-name $usertable || :
$AWS_CMD dynamodb wait table-not-exists \
    --endpoint-url $scylla_api \
    --table-name $usertable

nodes=(`ec2-ips scylla-db`)

# Unfortunately, Scylla is configured by default to keep snapshots
# (https://github.com/scylladb/scylladb/issues/5283). This can fill the
# disk when repeating the same test over and over. Let's delete the
# old snapshots.
case $aws in
"") # Scylla
    for n in ${nodes[@]}
    do
        ./ec2-ssh "$n" "sudo rm -rf /var/lib/scylla/data/alternator_$usertable/*/snapshots" &
    done
    wait
    ;;
esac


# set "wcu" (write capacity units) to create the table in provisioned billing
# mode.. without it, use on-demand billing mode.
if ! test -z "$wcu"
then
    billing_mode="--provisioned-throughput ReadCapacityUnits=$rcu,WriteCapacityUnits=$wcu"
else
    billing_mode="--billing-mode PAY_PER_REQUEST"
fi
$AWS_CMD dynamodb create-table \
    --endpoint-url $scylla_api \
    --table-name $usertable \
    --attribute-definitions \
        AttributeName=p,AttributeType=S \
    --key-schema \
        AttributeName=p,KeyType=HASH \
    $billing_mode \
    --tags Key=system:write_isolation,Value=$isolation
$AWS_CMD dynamodb wait table-exists \
    --endpoint-url $scylla_api \
    --table-name $usertable

# Number of YCSB loaders to use per loader/server combination.
# In other words, each loader will run MULT*nservers ycsb processes, and each
# server will be loaded by MULT*nloaders ycsb processes.
# TODO: I don't understand why with say fieldcount=1 MULT=2 it takes a very
# long time to get higher results than MULT=1.
MULT=${MULT-1}

TMPDIR=/tmp/ssh1-$$
#trap 'rm -r /tmp/ssh1-$$' 0 1 2 3 15
#trap 'rm -r /tmp/ssh1-$$' 1 2 3 15
mkdir $TMPDIR
echo keeping stats in $TMPDIR

maxexecutiontime=${time-300}

# Function to run a certain YCSB command on all loaders, and on each
# loader - one (or MULT) for each target node.
# The function to run, $1, is run with "eval" (watch out!) and can use the
# variables:
#   nloaders - total number of loaders to be run (#loaders * #nodes * MULT)
#   i - sequential number of loaders between 0 and nloaders
#   loader - loader address
#   node - node address
#   mult - sequential number between 0 and MULT counting YCSB runs for
#          same loader and node.
run_ycsb_on_all_loaders() {
    typeset -i i=0
    nloaders=$((${#loaders[@]}*${#nodes[@]}*$MULT))
    for loader in ${loaders[@]}
    do
        for node in ${nodes[@]}
        do
            case $aws in
            "") # Scylla:
                endpoint=http://$node:8080
                ;;
            *)  # AWS:
                endpoint=http://dynamodb.$aws.amazonaws.com
                ;;
            esac
            for mult in `seq $MULT`
            do
                # The ugly eval is to let the command ($1) interpolate
                # variables that are different in each run (e.g., $i)
                eval echo "\"$YCSB $1\""
                eval ec2-ssh $loader "\"$YCSB $1\"" > $TMPDIR/$loader-$node-$mult 2>&1 &
                let ++i
            done
        done
    done
    # Wait for all the loaders started above to fini
    # TODO: catch interrupt and kill all the loaders.
    wait
}

# For description of the following options, see
# https://github.com/brianfrankcooper/YCSB/blob/master/dynamodb/conf/dynamodb.properties
# NOTE: YCSB currently has two modes - "HASH" - with single-row
# partitions - and "HASH_AND_RANGE" - where we have a *single*
# partition and all the items in it. The HASH_AND_RANGE mode is
# worthless for benchmarking because it has a single hot
# partition.
#

case $workload in
a)
    # workloada is 50% reads, 50% updates of existing data, after a
    # write-only loading phase which writes all the data.
    # write nrecords per loader. Total writes are nloaders*nrecords.
    nrecords=${nrecords-10000}
    run_ycsb_on_all_loaders 'load dynamodb -P workloads/workloada -threads 100 \
            -p table=$usertable \
            -p dynamodb.endpoint=$endpoint \
            -p dynamodb.connectMax=200 \
            -p dynamodb.primaryKey=p \
            -p dynamodb.primaryKeyType=HASH \
            -p insertstart=$((i*nrecords)) \
            -p insertcount=$nrecords \
            -p recordcount=$((nloaders*nrecords)) \
            -p core_workload_insertion_retry_limit=10 \
            -p core_workload_insertion_retry_interval=1 \
            -s'
    # Uncomment this "scan" to verify that the load phase wrote the
    # expected number of items.
    $AWS_CMD dynamodb scan \
        --endpoint-url $scylla_api \
        --table-name $usertable --select COUNT
    run_ycsb_on_all_loaders 'run dynamodb -P workloads/workloada -threads 100 \
            -p table=$usertable \
            -p dynamodb.endpoint=$endpoint \
            -p dynamodb.connectMax=200 \
            -p dynamodb.consistentReads=true \
            -p dynamodb.primaryKey=p \
            -p dynamodb.primaryKeyType=HASH \
            -p recordcount=$((nloaders*nrecords)) \
            -p maxexecutiontime=$maxexecutiontime \
            -p operationcount=0 \
            -s'
    ;;
r)
    # read-only workload after a write-only loading phase which writes
    # nrecords per loader (total items are nloaders*nrecords).
    nrecords=${nrecords-10000}
    run_ycsb_on_all_loaders 'load dynamodb -P workloads/workloada -threads 100 \
            -p table=$usertable \
            -p dynamodb.endpoint=$endpoint \
            -p dynamodb.connectMax=200 \
            -p dynamodb.primaryKey=p \
            -p dynamodb.primaryKeyType=HASH \
            -p insertstart=$((i*nrecords)) \
            -p insertcount=$nrecords \
            -p recordcount=$((nloaders*nrecords)) \
            -p core_workload_insertion_retry_limit=10 \
            -p core_workload_insertion_retry_interval=1 \
            -s'
    # Uncomment this "scan" to verify that the load phase wrote the
    # expected number of items.
    #$AWS_CMD dynamodb scan \
    #    --endpoint-url $scylla_api \
    #    --table-name $usertable --select COUNT
    run_ycsb_on_all_loaders 'run dynamodb -P workloads/workloada -threads 100 \
            -p table=$usertable \
            -p dynamodb.endpoint=$endpoint \
            -p dynamodb.connectMax=200 \
            -p dynamodb.consistentReads=${consistent-true} \
            -p dynamodb.primaryKey=p \
            -p dynamodb.primaryKeyType=HASH \
            -p recordcount=$((nloaders*nrecords)) \
            -p maxexecutiontime=$maxexecutiontime \
            -p operationcount=0 \
            -p readproportion=1 \
            -p updateproportion=0 \
            -p scanproportion=0 \
            -p insertproportion=0 \
            -p requestdistribution=${distribution-zipfian} \
            -s'
    ;;
w|*)
    # Write-only workload until given $time.
    if ! test -z "$wcu"
    then
        # FIXME: need to divide by item size!
        target="-target $((wcu/nloaders))"
    else
        target=
    fi
    run_ycsb_on_all_loaders 'run dynamodb -P workloads/workloada -threads 100 \
            $target \
            -p table=$usertable \
            -p recordcount=10000000 \
            -p requestdistribution=uniform \
            -p fieldcount=${fieldcount-10} \
            -p fieldlength=${fieldlength-256} \
            -p readproportion=0 \
            -p updateproportion=0 \
            -p scanproportion=0 \
            -p insertproportion=1 \
            -p maxexecutiontime=$maxexecutiontime \
            -p operationcount=0 \
            -p measurementtype=hdrhistogram \
            -p dynamodb.endpoint=$endpoint \
            -p dynamodb.connectMax=200 \
            -p dynamodb.primaryKey=p \
            -p dynamodb.primaryKeyType=HASH \
            -s'
    ;;
esac

case $aws in
"") # Scylla
    # Leave the table behind. It has a fixed name and will be deleted in
    # the next run.
    ;;
*)  # AWS
    # On AWS, delete the table so we don't continue to pay a fortune for it
    # if it is a provisioned table!
    $AWS_CMD dynamodb delete-table \
        --endpoint-url $scylla_api \
        --table-name $usertable || :
    ;;
esac

# Add the average (over the entire run) of the throughput of each loader.
# For the result to make sense, the loaders should all start and stop at
# roughly the same time, and the run must be long enough:
# * "time" of 300 (5 minutes) seems to give around 95% of the throughput
# * "time" of 600 (10 minutes) seems to give around 98% of the throughput
# A more accurate approach would be to calculate # the total throughput at
# each second and then graph (and average) those, but that's more complicated.
set +x
fgrep -h '[OVERALL], Throughput(ops/sec)' $TMPDIR/* |
    awk '{sum+=$3} END {print "total throughput (incl. failures): " sum}'
fgrep -h '[INSERT], Return=OK' $TMPDIR/* |
    awk '{sum+=$3} END {print "successful INSERT throughput: " sum/'$maxexecutiontime'}'
fgrep -h '[READ], Return=OK' $TMPDIR/* |
    awk '{sum+=$3} END {print "successful READ throughput: " sum/'$maxexecutiontime'}'
fgrep -h '[UPDATE], Return=OK' $TMPDIR/* |
    awk '{sum+=$3} END {print "successful UPDATE throughput: " sum/'$maxexecutiontime'}'
